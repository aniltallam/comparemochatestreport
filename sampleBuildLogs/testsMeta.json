[
  {
    "file": "/extension/test.wrappers.js",
    "title": "should first invoke an export wrapper that return an array of arrays as data. Then it should pass that data to another import wrapper without any mappings ensure that its not mangled in any way.",
    "suite": [
      "Wrapper End to End Tests"
    ]
  },
  {
    "file": "/extension/test.wrappers.js",
    "title": "should first invoke an export wrapper that return an array of objects that contains arrays. Then it should pass that data to another import wrapper without any mappings and ensure that its not mangled in any way.",
    "suite": [
      "Wrapper End to End Tests"
    ]
  },
  {
    "file": "/extension/test.wrappers.js",
    "title": "should first invoke an export wrapper that return an array of arrays as data. Then it should pass that data to another import wrapper whose function doesn't exist and ensure that job error contains correct code and source.",
    "suite": [
      "Wrapper End to End Tests"
    ]
  },
  {
    "file": "/extension/test.wrappers.js",
    "title": "should create a flow which contains export of type wrapper and import of type wrapper. Now, invoke the flow by passing an export startDate in the body, validate the startDate field in the export wrapper function (deltaExportWithFixedStartDateFromRunFlow). Now, validate that job is successfully completed with success count of 1.",
    "suite": [
      "Wrapper End to End Tests"
    ]
  },
  {
    "file": "/qgmw/test.aggregation.js",
    "title": "should aggregate the results of a multi-page export into one CSV file and place that one file on an ftp site",
    "suite": [
      "Aggregation Tests"
    ]
  },
  {
    "file": "/qgmw/test.aggregation.js",
    "title": "should aggregate the results of a distributed export into one CSV file and place that one file on an ftp site",
    "suite": [
      "Aggregation Tests"
    ]
  },
  {
    "file": "/qgmw/test.cancel.js",
    "title": "should start a flow and then cancel it before it has a chance to start running anything",
    "suite": [
      "Cancel Flow Tests"
    ]
  },
  {
    "file": "/qgmw/test.cancel.js",
    "title": "should start a flow with errors, let it run for sometime, cancel it while running and check whether status after canceled remains as canceled while job error count and error file get updated for already executed records",
    "suite": [
      "Cancel Flow Tests"
    ]
  },
  {
    "file": "/qgmw/test.cancel.js",
    "title": "should trigger a realtime flow with errors, let it run for sometime, cancel it while running and check whether status after canceled remains as canceled while job error count and error file get updated for already executed records",
    "suite": [
      "Cancel Flow Tests"
    ]
  },
  {
    "file": "/qgmw/test.cancel.js",
    "title": "should run till flow job is canceled and export too should get canceled after that.",
    "suite": [
      "Cancel Flow Tests",
      "for a slow export"
    ]
  },
  {
    "file": "/qgmw/test.cancel.js",
    "title": "should start a run a flow with errors, retry it and cancel the retry job right after retrying. Then it should properly update the status of the jobs",
    "suite": [
      "Cancel Flow Tests"
    ]
  },
  {
    "file": "/qgmw/test.clearUnusedStacks.js",
    "title": "should create two stacks which are not being referred by any docs and are template based. The batch processshould delete the two stacks and also delete the lambda functions.",
    "suite": [
      "Clear unused template based stacks test"
    ]
  },
  {
    "file": "/qgmw/test.clearUnusedStacks.js",
    "title": "should create two stacks which are not being referred by any docs and are template based. Should post message \"clear_template_based_stacks\" to qgmw. Should check whether the stacks get deleted.",
    "suite": [
      "Clear unused template based stacks test"
    ]
  },
  {
    "file": "/qgmw/test.flowcache.js",
    "title": "should remove a flow from flowCache by removing its schedule property",
    "suite": [
      "Flow Cache Tests",
      "should start up some flows and then force them to get removed from the flowCache, this will verify that our system does not get corrupted because we were iterating over an array while changing its size"
    ]
  },
  {
    "file": "/qgmw/test.idLockTemplate.js",
    "title": "should run a flow that has very small pages and all duplicate records and make sure that the idLockTemplate logic prevents multiple requests from happening at the same time.  we also validate that without the idLockTemplate set the flow finishes much faster but also contains mostly all errors.",
    "suite": [
      "idLockTemplate Tests"
    ]
  },
  {
    "file": "/qgmw/test.idLockTemplate.js",
    "title": "should run a very nuanced flow to expose possible corner case bugs in the idLockTemplate logic.  basically we are going to import ids that collide accross pages and we are going to make sure that 2 rounds of pages are processed to make sure the idLockTemplate works when we exceed our concurrencyLevel",
    "suite": [
      "idLockTemplate Tests"
    ]
  },
  {
    "file": "/qgmw/test.jobProgress.js",
    "title": "should trigger a realtime flow with errors and validate job progress update",
    "suite": [
      "Job Progress Update Tests"
    ]
  },
  {
    "file": "/qgmw/test.jobProgress.js",
    "title": "should start a run a flow with errors, retry it and validate job progress update",
    "suite": [
      "Job Progress Update Tests"
    ]
  },
  {
    "file": "/qgmw/test.license.js",
    "title": "should create multiple users with different integrator licenses. should create variety of flows for each user. The license system batch process should disable the flows according to integrator license rules.",
    "suite": [
      "Integrator license batch process",
      "Tests"
    ]
  },
  {
    "file": "/qgmw/test.license.js",
    "title": "should create multiple users with different integrator licenses. Invoke the license system batch process for the licenses. The license system batch should disable the flows according to integrator license rules.",
    "suite": [
      "Integrator license batch process",
      "Tests"
    ]
  },
  {
    "file": "/qgmw/test.license.js",
    "title": "should ensure server log consists of licenseAuditProcess logs license information.",
    "suite": [
      "Integrator license batch process",
      "Tests"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should validate the correct job error message when a synchronous export is encountered by a worker",
    "suite": [
      "Miscellaneous Tests"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should validate whether ignored import responses are not included in success count",
    "suite": [
      "Miscellaneous Tests"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should create a flow with a distributed export and then post an empty payload to the flow and make sure the job does not get stuck in a running status.",
    "suite": [
      "Miscellaneous Tests"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should validate whether proper flow timestamp is passed into processImport method. This is done by running an FTP adaptor and validating whether created file has the timestamp appended",
    "suite": [
      "Miscellaneous Tests"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should validate that a flow job (where zero records are exported) gets marked completed.",
    "suite": [
      "Miscellaneous Tests"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should create a dataloader flow and uplaod csv file to s3, then run the flow and validate the completed job.",
    "suite": [
      "Miscellaneous Tests"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should keep updating the job progress and finally complete the job",
    "suite": [
      "Miscellaneous Tests",
      "for a slow export"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should properly run all parallelly using multiple consumer loops.",
    "suite": [
      "Miscellaneous Tests",
      "for consumer concurrency"
    ]
  },
  {
    "file": "/qgmw/test.misc.js",
    "title": "should successfully run a flow when 1) single record was exported and 2) multiple records were exported.",
    "suite": [
      "Miscellaneous Tests",
      "webhook exports"
    ]
  },
  {
    "file": "/qgmw/test.notifications.js",
    "title": "should create a connection and create the notifications for the owner of the connectionNow create a flow with the same connection. After the flow has had a chance to process at least one page of data bring the _connectionId of the flow offline by changing its bearerToken.This should cause errors mid-page processing. validate that the flow sharing this conection pause, and no further messages/pages are processed.  next validate that GET tiles shows an offline connection for the flow using the down _connectionId.  Now check whether the notifications are sent to the owner of the connection with the event_type \"connection_offline\"Now bring back the connection online, and check whether notifications with event_type \"connection_online\" are sent.",
    "suite": [
      "Notifications",
      "Notifications for the event type \"connection_online\" or \"connection_offline\""
    ]
  },
  {
    "file": "/qgmw/test.notifications.js",
    "title": "should create a flow with invalid records and create notifications of the integration for the owner. Now, run the flow and verify that email notification is sent to the owner.",
    "suite": [
      "Notifications",
      "Notifications for the event type \"job_errors\""
    ]
  },
  {
    "file": "/qgmw/test.notifications.js",
    "title": "should create a flow with an invalid connection and create notifications for the owner of the flow. Now, run the flow and verify that email job status becomes \"failed\" and email notification is sent.",
    "suite": [
      "Notifications",
      "Notifications for the event type \"job_errors\""
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should test frequent database downtime events",
    "suite": [
      "Things Going Offline Tests",
      "should start up a bunch of flows and validate that frequent/persistent downtime events do not disrupt flows from completing.  very important to validate that the results of flows are not affected at all by underlying components going down",
      "Mongo/Iron/S3 goes down"
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should test frequent Iron.io API downtime events",
    "suite": [
      "Things Going Offline Tests",
      "should start up a bunch of flows and validate that frequent/persistent downtime events do not disrupt flows from completing.  very important to validate that the results of flows are not affected at all by underlying components going down",
      "Mongo/Iron/S3 goes down"
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should test frequent Amazon S3 downtime events",
    "suite": [
      "Things Going Offline Tests",
      "should start up a bunch of flows and validate that frequent/persistent downtime events do not disrupt flows from completing.  very important to validate that the results of flows are not affected at all by underlying components going down",
      "Mongo/Iron/S3 goes down"
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should test that a QGWorker currently runnign flows can be cycled down without disrupting anything, and after the cycle down completes the flows should keep processing on another QGWorker instance.to a other QGMaster process",
    "suite": [
      "Things Going Offline Tests",
      "should start up a bunch of flows and validate that frequent/persistent downtime events do not disrupt flows from completing.  very important to validate that the results of flows are not affected at all by underlying components going down",
      "QGWorker offline tests"
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should test that a QGWorker abruptly taken offline will eventually get flagged as unhealthy such that no additional queues get assigned to it.",
    "suite": [
      "Things Going Offline Tests",
      "should start up a bunch of flows and validate that frequent/persistent downtime events do not disrupt flows from completing.  very important to validate that the results of flows are not affected at all by underlying components going down",
      "QGWorker offline tests"
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should test that a persistent QGMaster downtime event results in a switch to a backup QGMaster process",
    "suite": [
      "Things Going Offline Tests",
      "QGMaster offline tests"
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should start up multiple flows.  some of the flows should share the same _connectionId, and some of the flows should be completely independent.  after all flows have had a chance to process at least one page of data bring the shared _connectionId offline by changing its bearerToken.  IMPORTANT: update the accesstoken directly in the database to simulate a connection unexpectedly changing without any notice -- this should cause errors mid-page processing. validate that all the flows sharing this conection pause, and no further messages/pages are processed.  validate that the flows that are not using the down _connectionId come to completion.  next validate that GET tiles shows an offline connection for the flows using the down _connectionId.  now bring the _connectionId back online by updating the bearerToken to the new value that was set in the database.  IMPORTANT that this update be done with the API because there are built in notifications to tell the worker infrastructure to resume processing.  Basically, the update of the connection should be enough to let everything know to start running again.  validate that everything picks right back up and comes to a completion.  as a bonus run a retry on all the jobs with errors to validate that any connection errors are re-run and do not fail the second time around.  be sure to validate that you saw at least one connection error in the job errors.  the whole point of this test is to validate how integrator handles connections going down mid processing, and it is important to validate existence of this error before triggering a retry.",
    "suite": [
      "Things Going Offline Tests",
      "connection going offline"
    ]
  },
  {
    "file": "/qgmw/test.offline.js",
    "title": "should trigger a realtime flow whose import connection goes offline. Hence original job should be marked as completed and a new job needs to be created and processed once it become online.",
    "suite": [
      "Things Going Offline Tests"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "for flow daisy chaining using flow._runNextFlowIds. i.e. Once the parent job completed, flows referred by _runNextFlowIds should automatically get executed.",
    "suite": [
      "Orchestration test"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "for unrelated import chaining using flow.pageProcessors should execute all the imports in the array, creating proper retries.",
    "suite": [
      "Orchestration test"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "for unrelated import chaining using flow.pageProcessors should execute all the pageProcessors in the array, creating proper retries for a realtime flow.",
    "suite": [
      "Orchestration test"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "for async imports should verify the proper error, success and ignore count updates along with proper job errors and retries while retrying an async import is also verified. Further this address the usage of submit sub document when initial async response is different than the status response.",
    "suite": [
      "Orchestration test",
      "for async"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "imports should verify the proper error, success and ignore count updates along with proper job errors and retries while retrying an async import is also verified. Further this address the usage of submit sub document when initial async response is different than the status response.",
    "suite": [
      "Orchestration test",
      "for async realtime"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should run a flow with oneToMany import and verify proper numError, numSuccess, numIgnore counts along with retry execution.",
    "suite": [
      "Orchestration test",
      "for oneToMany"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should run a flow with pathToMany import and verify proper numError, numSuccess, numIgnore counts along with retry execution.",
    "suite": [
      "Orchestration test",
      "for oneToMany"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should run an orchestration flow with pathToMany imports and exports, verify proper numError, numSuccess, numIgnore counts along with retry execution.",
    "suite": [
      "Orchestration test",
      "for oneToMany"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should run an orchestration flow with pathToMany import and exports, without proceeding on failures and verify proper numError, numSuccess, numIgnore counts along with retry execution.",
    "suite": [
      "Orchestration test",
      "for oneToMany"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should test that retries work for large import chains with errors spanning multiple different import jobs at the same time, with multiple retry jobs running at the same time, etc...",
    "suite": [
      "Orchestration test",
      "complex retry scenarios for orchestration flow jobs"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should test that retries work for large import chains with proceedOnFailure set to true.",
    "suite": [
      "Orchestration test",
      "complex retry scenarios for orchestration flow jobs"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should test that retries are not allowed for jobs where the flow has been changed and there is no longer a match between the flow definition and the state of the job.",
    "suite": [
      "Orchestration test",
      "complex retry scenarios for orchestration flow jobs"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should run multiple exports and process records through same set of pageProcessors",
    "suite": [
      "Orchestration test",
      "for export daisy chaining"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should run multiple exports with their child exports and process records through same set of pageProcessors",
    "suite": [
      "Orchestration test",
      "for parent child exports"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should properly complete all the jobs when no content is exported",
    "suite": [
      "Orchestration test",
      "for parent child exports"
    ]
  },
  {
    "file": "/qgmw/test.orchestration.js",
    "title": "should child exports and process records through same set of pageProcessors",
    "suite": [
      "Orchestration test",
      "for realtime parent child exports"
    ]
  },
  {
    "file": "/qgmw/test.pingOfflineConnections.js",
    "title": "should start a flow.after the flow has had a chance to process at least one page of data bring the _connectionId of the flow offline by changing its bearerToken.  IMPORTANT: update the accesstoken directly in the database to simulate a connection unexpectedly changing without any notice -- this should cause errors mid-page processing. validate that the flow sharing this conection pause, and no further messages/pages are processed.  next validate that GET tiles shows an offline connection for the flow using the down _connectionId.  now post a message to the IronMQ queue \"ping_offline_connections\". This should bring back the connection online.validate that everything picks right back up and comes to a completion.",
    "suite": [
      "Ping Offline connections related tests"
    ]
  },
  {
    "file": "/qgmw/test.pingOfflineConnections.js",
    "title": "should start a flow.after the flow has had a chance to process at least one page of data bring the _connectionId of the flow offline by changing its bearerToken.  IMPORTANT: update the accesstoken directly in the database to simulate a connection unexpectedly changing without any notice -- this should cause errors mid-page processing. validate that the flow sharing this conection pause, and no further messages/pages are processed.  next validate that GET tiles shows an offline connection for the flow using the down _connectionId.  now post a message to update  pingOfflineConnectionsLastRunTime to 1 min.This should trigger the ping offline connections process within 1 minute and should bring back the connection online.validate that everything picks right back up and comes to a completion.",
    "suite": [
      "Ping Offline connections related tests"
    ]
  },
  {
    "file": "/qgmw/test.pingOfflineConnections.js",
    "title": "should create 150 offline connections, then post a message to theIronMQ queue \"ping_offline_connections\", this should trigger the ping offline connections process.After some time, verify that all connections are back online using retry logic.",
    "suite": [
      "Ping Offline connections related tests"
    ]
  },
  {
    "file": "/qgmw/test.pingOfflineConnections.js",
    "title": "should create an offline connection with an invalid api token and post a message to \"ping_offline_connections\" queue. Now, validate the ping process related fields in the connection. Now, again post the message to the queue with the invalid token and check whether retry count is updated correctly. After this, update the connection with the correct api token and post a message again to the queue. Now, ensure that the connection has come online and ping process related fields are updated to null.",
    "suite": [
      "Ping Offline connections related tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "this is an aweseome test, though it was excruciatinig to write.  it will run a flow that has both errors and successes (random distribution) and it will validate that the retry objects match the records that failed.  itwill invoke retry on the job without changing any of the retry data to validate that if nothing is changed the same errors will happen again.  if only a couple retry objects are manually fixed on the job then a retry should increment and decrement the job accordingly. and if all the retry objects on the job are manually fixed then the job should show as a success without any errors.  finally it will validate that if/when a job has zero errors and a retry is invoked then a 422 will be returned.",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should create a job with errors, download the error csv, mark a few errors to be resolved/retried, upload using a signed s3 url, preview the summary, retry based on uploaded csv, validate results, fix retry data, trigger a new csv retry and validate results",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should trigger a realtime flow with errors and validate job error file creation",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should upload an error csv file with millions of records, allow retries to be processed and verify memory growth",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should set a retriable flag for the jobs that can be retried and unset once it is not retriable while setting lastExecutedAt to the last retry endedAt value",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should check whether retries are created by default and not created whenflow has skipRetries flag set to true",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should start a flow with errors, let it run for sometime, cancel it while running and check the job status becomes \"canceled\" with some errors. Now, mark job errors in the error file as resolved and retry the job. Now, check that job status becomes \"completed\" with zero error count.",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should trigger a realtime flow which is mapped to an invalid import doc and check the flow run has completed with errors. Now, check the retry doc has dataURIs populated in it.",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "fail retry job when invalid or non-existing error files are referred",
    "suite": [
      "Retry Job Tests",
      "should"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should create a job which has multiple errors associated with a single retry id and check error resolution and retrying behaviour",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should validate the existense of exportDataURI and importDataURI in the job error file when a job with dataURI template is run or retried",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should create an integration with error flows and verify bulk error resolution by _jobIds, _flowId and _integrationId",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should validate retry file downloading for retries with type 'file'",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.retry.js",
    "title": "should successfully complete a retry job with deleted parent jobs",
    "suite": [
      "Retry Job Tests"
    ]
  },
  {
    "file": "/qgmw/test.uncaught.js",
    "title": "should run multiple flows and validate that one flow with uncaught exceptions does not bring the other flows down",
    "suite": [
      "Uncaught Exception Tests"
    ]
  },
  {
    "file": "/qgmw/test.usage.js",
    "title": "should start a DIY flow and then allow it to finish and then check that usage was logged correctly",
    "suite": [
      "Usage Tests"
    ]
  },
  {
    "file": "/qgmw/test.usage.js",
    "title": "should invoke DIY syncronous exports and imports and ping connection and then check that usage was logged correctly",
    "suite": [
      "Usage Tests"
    ]
  },
  {
    "file": "/qgmw/test.usage.js",
    "title": "should validate 401 when regular user attempts to access CONNECTOR usage APIs",
    "suite": [
      "Usage Tests"
    ]
  },
  {
    "file": "/qgmw/test.usage.js",
    "title": "should start a CONNECTOR flow and then allow it to finish and then check that usage was logged correctly",
    "suite": [
      "Usage Tests"
    ]
  },
  {
    "file": "/qgmw/test.usage.js",
    "title": "should invoke CONNECTOR syncronous exports and imports and ping connection and then check that usage was logged correctly",
    "suite": [
      "Usage Tests"
    ]
  },
  {
    "file": "/qgmw/test.usage.js",
    "title": "should validate whether retry usage is tracked when skipRetries is set to false",
    "suite": [
      "Usage Tests"
    ]
  }
]
