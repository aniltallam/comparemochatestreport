<!DOCTYPE html><html><head><style>table, th, td {border: 1px solid lightgrey;border-collapse:collapse;}</style></head><body><table style="width:100%"><tr><th>#</th><th>File</th><th>Test Name</th><th>Durs</th><th>Change</th></tr>
<tr><td>1</td><td>/qgmw/test.offline.js</td><td>should trigger a realtime flow whose import connection goes offline. Hence original job should be marked as completed and a new job needs to be created and processed once it become online.</td><td>[2.13,2.15]</td><td>0.7704943418502808 %</td></tr><tr><td>2</td><td>/qgmw/test.offline.js</td><td>should test frequent database downtime events</td><td>[4.97,4.53]</td><td>-8.89349365234375 %</td></tr><tr><td>3</td><td>/qgmw/test.offline.js</td><td>should test frequent Iron.io API downtime events</td><td>[5.25,4.58]</td><td>-12.74274730682373 %</td></tr><tr><td>4</td><td>/qgmw/test.offline.js</td><td>should test frequent Amazon S3 downtime events</td><td>[5.31,5.24]</td><td>-1.351287841796875 %</td></tr><tr><td>5</td><td>/qgmw/test.offline.js</td><td>should test that a QGWorker abruptly taken offline will eventually get flagged as unhealthy such that no additional queues get assigned to it.</td><td>[9.71,9.01]</td><td>-7.225917339324951 %</td></tr><tr><td>6</td><td>/qgmw/test.offline.js</td><td>should test that a persistent QGMaster downtime event results in a switch to a backup QGMaster process</td><td>[11.26,11.41]</td><td>1.3596608638763428 %</td></tr><tr><td>7</td><td>/qgmw/test.offline.js</td><td>should start up multiple flows.  some of the flows should share the same _connectionId, and some of the flows should be completely independent.  after all flows have had a chance to process at least one page of data bring the shared _connectionId offline by changing its bearerToken.  IMPORTANT: update the accesstoken directly in the database to simulate a connection unexpectedly changing without any notice -- this should cause errors mid-page processing. validate that all the flows sharing this conection pause, and no further messages/pages are processed.  validate that the flows that are not using the down _connectionId come to completion.  next validate that GET tiles shows an offline connection for the flows using the down _connectionId.  now bring the _connectionId back online by updating the bearerToken to the new value that was set in the database.  IMPORTANT that this update be done with the API because there are built in notifications to tell the worker infrastructure to resume processing.  Basically, the update of the connection should be enough to let everything know to start running again.  validate that everything picks right back up and comes to a completion.  as a bonus run a retry on all the jobs with errors to validate that any connection errors are re-run and do not fail the second time around.  be sure to validate that you saw at least one connection error in the job errors.  the whole point of this test is to validate how integrator handles connections going down mid processing, and it is important to validate existence of this error before triggering a retry.</td><td>[11.84]</td><td>NaN %</td></tr><tr><td>8</td><td>/qgmw/test.license.js</td><td>should create multiple users with different integrator licenses. should create variety of flows for each user. The license system batch process should disable the flows according to integrator license rules.</td><td>[1.01,1.01]</td><td>-0.11369253695011139 %</td></tr><tr><td>9</td><td>/qgmw/test.license.js</td><td>should create multiple users with different integrator licenses. Invoke the license system batch process for the licenses. The license system batch should disable the flows according to integrator license rules.</td><td>[0.51,0.5]</td><td>-0.8213484883308411 %</td></tr><tr><td>10</td><td>/qgmw/test.license.js</td><td>should ensure server log consists of licenseAuditProcess logs license information.</td><td>[0.5,0.5]</td><td>-0.043106306344270706 %</td></tr><tr><td>11</td><td>/qgmw/test.clearUnusedStacks.js</td><td>should create two stacks which are not being referred by any docs and are template based. The batch processshould delete the two stacks and also delete the lambda functions.</td><td>[0.51,0.51]</td><td>0.10556875169277191 %</td></tr><tr><td>12</td><td>/qgmw/test.clearUnusedStacks.js</td><td>should create two stacks which are not being referred by any docs and are template based. Should post message "clear_template_based_stacks" to qgmw. Should check whether the stacks get deleted.</td><td>[0.67,0.5]</td><td>-24.990039825439453 %</td></tr><tr><td>13</td><td>/qgmw/test.jobProgress.js</td><td>should trigger a realtime flow with errors and validate job progress update</td><td>[5.64,3.9]</td><td>-30.881254196166992 %</td></tr><tr><td>14</td><td>/qgmw/test.jobProgress.js</td><td>should start a run a flow with errors, retry it and validate job progress update</td><td>[5.39,3.93]</td><td>-27.14935874938965 %</td></tr><tr><td>15</td><td>/qgmw/test.retry.js</td><td>this is an aweseome test, though it was excruciatinig to write.  it will run a flow that has both errors and successes (random distribution) and it will validate that the retry objects match the records that failed.  itwill invoke retry on the job without changing any of the retry data to validate that if nothing is changed the same errors will happen again.  if only a couple retry objects are manually fixed on the job then a retry should increment and decrement the job accordingly. and if all the retry objects on the job are manually fixed then the job should show as a success without any errors.  finally it will validate that if/when a job has zero errors and a retry is invoked then a 422 will be returned.</td><td>[8.17]</td><td>NaN %</td></tr><tr><td>16</td><td>/qgmw/test.retry.js</td><td>should trigger a realtime flow with errors and validate job error file creation</td><td>[3.47,3.11]</td><td>-10.256989479064941 %</td></tr><tr><td>17</td><td>/qgmw/test.retry.js</td><td>should set a retriable flag for the jobs that can be retried and unset once it is not retriable while setting lastExecutedAt to the last retry endedAt value</td><td>[5.1,4.33]</td><td>-15.242476463317871 %</td></tr><tr><td>18</td><td>/qgmw/test.retry.js</td><td>should check whether retries are created by default and not created whenflow has skipRetries flag set to true</td><td>[5.55,4.4]</td><td>-20.68810272216797 %</td></tr><tr><td>19</td><td>/qgmw/test.retry.js</td><td>should trigger a realtime flow which is mapped to an invalid import doc and check the flow run has completed with errors. Now, check the retry doc has dataURIs populated in it.</td><td>[1.63,1.6]</td><td>-2.1648380756378174 %</td></tr><tr><td>20</td><td>/qgmw/test.retry.js</td><td>should validate the existense of exportDataURI and importDataURI in the job error file when a job with dataURI template is run or retried</td><td>[4.63]</td><td>NaN %</td></tr><tr><td>21</td><td>/qgmw/test.retry.js</td><td>should validate retry file downloading for retries with type 'file'</td><td>[3.96,3.13]</td><td>-21.016298294067383 %</td></tr><tr><td>22</td><td>/qgmw/test.retry.js</td><td>fail retry job when invalid or non-existing error files are referred</td><td>[3.55]</td><td>NaN %</td></tr><tr><td>23</td><td>/extension/test.wrappers.js</td><td>should first invoke an export wrapper that return an array of arrays as data. Then it should pass that data to another import wrapper without any mappings ensure that its not mangled in any way.</td><td>[1.76,1.8]</td><td>1.8255175352096558 %</td></tr><tr><td>24</td><td>/extension/test.wrappers.js</td><td>should first invoke an export wrapper that return an array of objects that contains arrays. Then it should pass that data to another import wrapper without any mappings and ensure that its not mangled in any way.</td><td>[2.08,1.72]</td><td>-17.252988815307617 %</td></tr><tr><td>25</td><td>/extension/test.wrappers.js</td><td>should first invoke an export wrapper that return an array of arrays as data. Then it should pass that data to another import wrapper whose function doesn't exist and ensure that job error contains correct code and source.</td><td>[2.35,2.23]</td><td>-4.894906520843506 %</td></tr><tr><td>26</td><td>/extension/test.wrappers.js</td><td>should create a flow which contains export of type wrapper and import of type wrapper. Now, invoke the flow by passing an export startDate in the body, validate the startDate field in the export wrapper function (deltaExportWithFixedStartDateFromRunFlow). Now, validate that job is successfully completed with success count of 1.</td><td>[1.88,1.83]</td><td>-2.469113826751709 %</td></tr><tr><td>27</td><td>/qgmw/test.aggregation.js</td><td>should aggregate the results of a multi-page export into one CSV file and place that one file on an ftp site</td><td>[5.37,4.18]</td><td>-22.160686492919922 %</td></tr><tr><td>28</td><td>/qgmw/test.aggregation.js</td><td>should aggregate the results of a distributed export into one CSV file and place that one file on an ftp site</td><td>[3.88,3.07]</td><td>-20.884246826171875 %</td></tr><tr><td>29</td><td>/qgmw/test.flowcache.js</td><td>should remove a flow from flowCache by removing its schedule property</td><td>[3.46,3.33]</td><td>-3.778841733932495 %</td></tr><tr><td>30</td><td>/qgmw/test.idLockTemplate.js</td><td>should run a very nuanced flow to expose possible corner case bugs in the idLockTemplate logic.  basically we are going to import ids that collide accross pages and we are going to make sure that 2 rounds of pages are processed to make sure the idLockTemplate works when we exceed our concurrencyLevel</td><td>[7.33]</td><td>NaN %</td></tr><tr><td>31</td><td>/qgmw/test.misc.js</td><td>should validate the correct job error message when a synchronous export is encountered by a worker</td><td>[3.73,3.72]</td><td>-0.2933441698551178 %</td></tr><tr><td>32</td><td>/qgmw/test.misc.js</td><td>should validate whether ignored import responses are not included in success count</td><td>[3.6,3.55]</td><td>-1.2950812578201294 %</td></tr><tr><td>33</td><td>/qgmw/test.misc.js</td><td>should create a flow with a distributed export and then post an empty payload to the flow and make sure the job does not get stuck in a running status.</td><td>[1.5,1.73]</td><td>15.895551681518555 %</td></tr><tr><td>34</td><td>/qgmw/test.misc.js</td><td>should validate whether proper flow timestamp is passed into processImport method. This is done by running an FTP adaptor and validating whether created file has the timestamp appended</td><td>[5.44,5.26]</td><td>-3.329690456390381 %</td></tr><tr><td>35</td><td>/qgmw/test.misc.js</td><td>should validate that a flow job (where zero records are exported) gets marked completed.</td><td>[1.46,1.7]</td><td>16.200626373291016 %</td></tr><tr><td>36</td><td>/qgmw/test.misc.js</td><td>should create a dataloader flow and uplaod csv file to s3, then run the flow and validate the completed job.</td><td>[4.59,4.67]</td><td>1.8659659624099731 %</td></tr><tr><td>37</td><td>/qgmw/test.misc.js</td><td>should keep updating the job progress and finally complete the job</td><td>[5.81,5.97]</td><td>2.7728865146636963 %</td></tr><tr><td>38</td><td>/qgmw/test.misc.js</td><td>should properly run all parallelly using multiple consumer loops.</td><td>[3.36,3.19]</td><td>-5.048524379730225 %</td></tr><tr><td>39</td><td>/qgmw/test.misc.js</td><td>should successfully run a flow when 1) single record was exported and 2) multiple records were exported.</td><td>[2.72,2.61]</td><td>-3.7696611881256104 %</td></tr><tr><td>40</td><td>/qgmw/test.pingOfflineConnections.js</td><td>should start a flow.after the flow has had a chance to process at least one page of data bring the _connectionId of the flow offline by changing its bearerToken.  IMPORTANT: update the accesstoken directly in the database to simulate a connection unexpectedly changing without any notice -- this should cause errors mid-page processing. validate that the flow sharing this conection pause, and no further messages/pages are processed.  next validate that GET tiles shows an offline connection for the flow using the down _connectionId.  now post a message to the IronMQ queue "ping_offline_connections". This should bring back the connection online.validate that everything picks right back up and comes to a completion.</td><td>[4.92,4.82]</td><td>-1.9723986387252808 %</td></tr><tr><td>41</td><td>/qgmw/test.pingOfflineConnections.js</td><td>should start a flow.after the flow has had a chance to process at least one page of data bring the _connectionId of the flow offline by changing its bearerToken.  IMPORTANT: update the accesstoken directly in the database to simulate a connection unexpectedly changing without any notice -- this should cause errors mid-page processing. validate that the flow sharing this conection pause, and no further messages/pages are processed.  next validate that GET tiles shows an offline connection for the flow using the down _connectionId.  now post a message to update  pingOfflineConnectionsLastRunTime to 1 min.This should trigger the ping offline connections process within 1 minute and should bring back the connection online.validate that everything picks right back up and comes to a completion.</td><td>[4.81,5.04]</td><td>4.687045574188232 %</td></tr><tr><td>42</td><td>/qgmw/test.pingOfflineConnections.js</td><td>should create an offline connection with an invalid api token and post a message to "ping_offline_connections" queue. Now, validate the ping process related fields in the connection. Now, again post the message to the queue with the invalid token and check whether retry count is updated correctly. After this, update the connection with the correct api token and post a message again to the queue. Now, ensure that the connection has come online and ping process related fields are updated to null.</td><td>[4.43,4.41]</td><td>-0.44997382164001465 %</td></tr><tr><td>43</td><td>/qgmw/test.cancel.js</td><td>should start a flow and then cancel it before it has a chance to start running anything</td><td>[2.27,2.36]</td><td>3.664456844329834 %</td></tr><tr><td>44</td><td>/qgmw/test.cancel.js</td><td>should trigger a realtime flow with errors, let it run for sometime, cancel it while running and check whether status after canceled remains as canceled while job error count and error file get updated for already executed records</td><td>[4.31]</td><td>NaN %</td></tr><tr><td>45</td><td>/qgmw/test.cancel.js</td><td>should start a run a flow with errors, retry it and cancel the retry job right after retrying. Then it should properly update the status of the jobs</td><td>[2.99,3.32]</td><td>10.777119636535645 %</td></tr><tr><td>46</td><td>/qgmw/test.cancel.js</td><td>should run till flow job is canceled and export too should get canceled after that.</td><td>[5.62,5.46]</td><td>-2.823317289352417 %</td></tr><tr><td>47</td><td>/qgmw/test.usage.js</td><td>should start a DIY flow and then allow it to finish and then check that usage was logged correctly</td><td>[5.24,5.4]</td><td>3.086531639099121 %</td></tr><tr><td>48</td><td>/qgmw/test.usage.js</td><td>should invoke DIY syncronous exports and imports and ping connection and then check that usage was logged correctly</td><td>[0.96,0.94]</td><td>-1.93830144405365 %</td></tr><tr><td>49</td><td>/qgmw/test.usage.js</td><td>should validate 401 when regular user attempts to access CONNECTOR usage APIs</td><td>[0.02,0.01]</td><td>-38.54389572143555 %</td></tr><tr><td>50</td><td>/qgmw/test.usage.js</td><td>should start a CONNECTOR flow and then allow it to finish and then check that usage was logged correctly</td><td>[6.4,6]</td><td>-6.2423481941223145 %</td></tr><tr><td>51</td><td>/qgmw/test.usage.js</td><td>should invoke CONNECTOR syncronous exports and imports and ping connection and then check that usage was logged correctly</td><td>[0.93,0.99]</td><td>6.640701770782471 %</td></tr><tr><td>52</td><td>/qgmw/test.usage.js</td><td>should validate whether retry usage is tracked when skipRetries is set to false</td><td>[7.8,7.61]</td><td>-2.401935338973999 %</td></tr><tr><td>53</td><td>/qgmw/test.orchestration.js</td><td>for unrelated import chaining using flow.pageProcessors should execute all the imports in the array, creating proper retries.</td><td>[4.8,4.84]</td><td>0.6816809773445129 %</td></tr><tr><td>54</td><td>/qgmw/test.orchestration.js</td><td>for unrelated import chaining using flow.pageProcessors should execute all the pageProcessors in the array, creating proper retries for a realtime flow.</td><td>[6.82,6.83]</td><td>0.16086289286613464 %</td></tr><tr><td>55</td><td>/qgmw/test.orchestration.js</td><td>imports should verify the proper error, success and ignore count updates along with proper job errors and retries while retrying an async import is also verified. Further this address the usage of submit sub document when initial async response is different than the status response.</td><td>[8.38,8.54]</td><td>1.9148592948913574 %</td></tr><tr><td>56</td><td>/qgmw/test.orchestration.js</td><td>should run a flow with oneToMany import and verify proper numError, numSuccess, numIgnore counts along with retry execution.</td><td>[4.29,4.59]</td><td>6.921762943267822 %</td></tr><tr><td>57</td><td>/qgmw/test.orchestration.js</td><td>should run a flow with pathToMany import and verify proper numError, numSuccess, numIgnore counts along with retry execution.</td><td>[4.43,4.41]</td><td>-0.4275836646556854 %</td></tr><tr><td>58</td><td>/qgmw/test.orchestration.js</td><td>should run an orchestration flow with pathToMany import and exports, without proceeding on failures and verify proper numError, numSuccess, numIgnore counts along with retry execution.</td><td>[5.33,5.44]</td><td>2.019131660461426 %</td></tr><tr><td>59</td><td>/qgmw/test.orchestration.js</td><td>should test that retries work for large import chains with errors spanning multiple different import jobs at the same time, with multiple retry jobs running at the same time, etc...</td><td>[6.57,6.91]</td><td>5.200516223907471 %</td></tr><tr><td>60</td><td>/qgmw/test.orchestration.js</td><td>should test that retries are not allowed for jobs where the flow has been changed and there is no longer a match between the flow definition and the state of the job.</td><td>[5.69,5.66]</td><td>-0.4341440796852112 %</td></tr><tr><td>61</td><td>/qgmw/test.orchestration.js</td><td>should run multiple exports and process records through same set of pageProcessors</td><td>[5.52,5.19]</td><td>-5.969306945800781 %</td></tr><tr><td>62</td><td>/qgmw/test.orchestration.js</td><td>should run multiple exports with their child exports and process records through same set of pageProcessors</td><td>[7.3,7.26]</td><td>-0.5960819721221924 %</td></tr><tr><td>63</td><td>/qgmw/test.orchestration.js</td><td>should properly complete all the jobs when no content is exported</td><td>[3.58,4.4]</td><td>22.866222381591797 %</td></tr><tr><td>64</td><td>/qgmw/test.orchestration.js</td><td>should child exports and process records through same set of pageProcessors</td><td>[6.78,6.87]</td><td>1.3217949867248535 %</td></tr><tr><td>65</td><td>/qgmw/test.uncaught.js</td><td>should run multiple flows and validate that one flow with uncaught exceptions does not bring the other flows down</td><td>[7.16,7.39]</td><td>3.1968047618865967 %</td></tr><tr><td>66</td><td>/qgmw/test.notifications.js</td><td>should create a connection and create the notifications for the owner of the connectionNow create a flow with the same connection. After the flow has had a chance to process at least one page of data bring the _connectionId of the flow offline by changing its bearerToken.This should cause errors mid-page processing. validate that the flow sharing this conection pause, and no further messages/pages are processed.  next validate that GET tiles shows an offline connection for the flow using the down _connectionId.  Now check whether the notifications are sent to the owner of the connection with the event_type "connection_offline"Now bring back the connection online, and check whether notifications with event_type "connection_online" are sent.</td><td>[4.46,4.83]</td><td>8.471920013427734 %</td></tr><tr><td>67</td><td>/qgmw/test.notifications.js</td><td>should create a flow with invalid records and create notifications of the integration for the owner. Now, run the flow and verify that email notification is sent to the owner.</td><td>[2.25,2.36]</td><td>4.912140369415283 %</td></tr><tr><td>68</td><td>/qgmw/test.notifications.js</td><td>should create a flow with an invalid connection and create notifications for the owner of the flow. Now, run the flow and verify that email job status becomes "failed" and email notification is sent.</td><td>[2.01,2.31]</td><td>14.983797073364258 %</td></tr><tr><td>69</td><td>/qgmw/test.retry.js</td><td>should create a job with errors, download the error csv, mark a few errors to be resolved/retried, upload using a signed s3 url, preview the summary, retry based on uploaded csv, validate results, fix retry data, trigger a new csv retry and validate results</td><td>[4.24]</td><td>NaN %</td></tr><tr><td>70</td><td>/qgmw/test.retry.js</td><td>should start a flow with errors, let it run for sometime, cancel it while running and check the job status becomes "canceled" with some errors. Now, mark job errors in the error file as resolved and retry the job. Now, check that job status becomes "completed" with zero error count.</td><td>[4.08]</td><td>NaN %</td></tr><tr><td>71</td><td>/qgmw/test.retry.js</td><td>should create an integration with error flows and verify bulk error resolution by _jobIds, _flowId and _integrationId</td><td>[7.93,9.02]</td><td>13.818257331848145 %</td></tr><tr><td>72</td><td>/qgmw/test.idLockTemplate.js</td><td>should run a flow that has very small pages and all duplicate records and make sure that the idLockTemplate logic prevents multiple requests from happening at the same time.  we also validate that without the idLockTemplate set the flow finishes much faster but also contains mostly all errors.</td><td>[5.7]</td><td>NaN %</td></tr><tr><td>73</td><td>undefined</td><td>should borrow concurrency from a connection with higher concurrency and completed sooner</td><td>[6.56]</td><td>NaN %</td></tr><tr><td>74</td><td>undefined</td><td>2017-09-21T03:01:35.518Z - info:     ��� for flow daisy chaining using flow._runNextFlowIds. i.e. Once the parent job completed, flows referred by _runNextFlowIds should automatically get executed.</td><td>[4.1]</td><td>NaN %</td></tr><tr><td>75</td><td>/qgmw/test.orchestration.js</td><td>should run an orchestration flow with pathToMany imports and exports, verify proper numError, numSuccess, numIgnore counts along with retry execution.</td><td>[5.12]</td><td>NaN %</td></tr><tr><td>76</td><td></td><td>Summary of comparison</td><td>[265.25,258.12]</td><td>-2.687645435333252 %</td></tr></table></body></html>